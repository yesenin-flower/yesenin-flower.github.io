# Adversary Resistant Deep Neural Networks with an Application to Malware Detection
## 问题：
“blind spots” DNNs are vulnerable to adversarial samples, a shortcoming that plagues most, if not all, statistical and machine learning models

(作者好狡猾：In this work, we do not address this issue by training a DNN model that covers the entire adversarial space. Rather, our design principle is to increase the difficulty for adversaries in finding the adversarial space efficiently.)

## 以前的解决方法：
augmenting training data：Expand the training set by combining known samples with potential blind spots.但是高维数据可能的blind spots很多，很难覆盖全；+正则项(keywords: DataGrad)到loss里，惩罚扰动没覆盖的方向，这种方法高效生成sample但并不能保证覆盖到所有blind spots

enhancing model complexity：improve the tolerance of complex DNN models with respect to adversarial samples generated from simple DNN models. defensive distillation mechanism, which trains a DNN from data samples that are “distilled” from another DNN. 可以通过训练两个其他共享相同功能并具有类似性能的其他DNN模型被攻击者模仿，一旦学习了两个逼近DNN模型，攻击者就可以生成专门针对该提取增强DNN模型的敌对样本。但还是可以被approximated and exploited.

## 以前的解决方法缺陷：
cannot provide theoretical guarantees of robustness against adversarial sampled-based attacks

任何输入样本的梯度表示该高维输入空间中的方向向量。（只有一个） 沿着这个方向，这个输入样本的任何小的变化都会导致DNN产生完全不同的预测结果。 这个特定的方向非常重要，因为它代表了降低DNN性能的最有效的方法。 发现这个特定的方向是通过将来自输出层的层梯度通过错误的反向传播一直传回到输入层来完成的。 然后可以将输入的梯度应用于输入样本以制作对抗性示例。

## 论文方案：
RANDOM FEATURE NULLIFICATION

一个mask随机给一些pixels置空。在输入和第一隐藏层之间引入了附加层。 这个中间层是随机的，在训练和测试阶段都是随机性的来源。 用图像识别举例，当DNN将图像样本传递通过图层时，它会随机删除图像中的一些像素（每一个神经元删除的pixel不同，数量也不同），然后将部分损坏的图像提供给第一个隐藏层。置空层的权重也通过求导计算。每次训练样本，随机挑选的Ipi在向前和向后传播期间固定，直到下一个训练样本到达。这使得可以计算L(f（xi，Ipi;θ），yi) 关于θ的导数并相应地更新θ。在测试过程中，当模型参数固定时，为了得到稳定的测试结果，我们使用高斯分布N（μp，σp2）作为随机变量pi的替代值。

想多了，作者的实现并没有那么复杂。见RFN.py

## 论文实验：
跟adversarial training and dropout比较。把论文方案加入到adversarial training中，比较加入后的效果
数据：10000维，01表示调用事件，14399个benign 11679个malicious 3000test benign  3000test malicious


数据处理注意！

1. restrict the total number of manipulations that can occur per malware sample to be as small as possible.因为large-scale manipulations across all features may break down a malicious program’s functionality.
2. since removing certain file system calls may also jeopardize a malware’s internal logic, we further restrict the manipulation by only allowing the addition of new file system accesses.
3. only generate adversarial samples from the malware data points.

Evaluation

Resistance是怎么算的：输入恶意软件，求导得到delta t，加到恶意软件上得到全量的adversarial sample，再用自己model跑出adversarial sample，求比

## 总结
1. 文章的目的可以作为子方向参考，但不是我们的论文目的
2. 这个resistance的评估方法很有用，以后肯定会用到。另外，论文对数据的处理要用到，尤其是只加API不减，限制manipulation的次数。我们也要加入正则，或者变异后去掉manipulation的次数过多的。
3. 论文的缺陷在于，恶意软件的调用序列不仅仅是增加一些调用，还有重复调用无关API。可能作者只考虑了adversarial sample的特点
4. 其实认真想一下，rfu相当于是把原数据X进行了一个变异，再输入到DNN。可能是作者方法是可导的，所以才把这个变异作为一层，揉到网络里。如果我们的方法也可导，自然也可以把他融入到网络里。
5. [可以参考一下](https://blog.csdn.net/u014314005/article/details/80585338)

#Adversarial Attacks on Face Detectors using Neural Net based Constrained Optimization

## 问题：

“blind spots”

## 以前的解决方法：

Fast Gradient Sign Method | L-BFGS | Jacobian-based Saliency Map Attack (JSMA) | DeepFool | Carlini-Wagner 白盒方法，需要知道分类器的细节，而且慢

Keywords: Fast Gradient Sign Method | Carlini-Wagner | Adversarial Transformative Networks | Dense Adversary Generation 

## 论文方案：

offline训练一个generator，+Small perturbations。 Generating an adversarial example is fast and inexpensive, even more so than for FGSM, since creating a perturbation for an input only requires a forward pass once the generator is sufficiently well-trained.

loss: (x)+ = max(x, 0)

![](https://ws2.sinaimg.cn/large/006tNc79ly1fsvpnbk8bjj30mi034dg1.jpg)

![](https://ws1.sinaimg.cn/large/006tNc79ly1fsvpqgqz9xj315c0e6gnm.jpg)

![](https://ws1.sinaimg.cn/large/006tNc79ly1fsvprqdipyj30m00r6jv5.jpg)

## 论文实验：

Fast R-CNN预训练

1. 比较不同方法的时间，注意他只比较测试时间
2. 不同置信度，用Fast R-CNN识别出的人脸，用我们方法后识别出的人脸

## 总结

1. 论文的目标是生成generator，这样性能也有提高。我们可以考虑这一点，比如我们可以强调有了一个新的恶意样本，我们可以通过这个generator生成很多新样本，从而提高我们detector效果
2. 论文related work总结的很好，我们可以也需要提一下这几个。尤其是Adversarial Trans- formation Network 和 Carlini-Wagner
3. 论文没有复杂的理论，都是别人的方法，就连loss都是已有的

# Black-Box Attacks against RNN based Malware Detection Algorithms

## 问题：

针对 RNN based malware detection system generate sequential adversarial examples.

也是API序列，插入无关的APIs到原序列中

## 以前的解决方法：

Szegedy et al. used a box-constrained L-BFGS to search for an appropriate perturbation which can make a neural network misclassify an image

Goodfellow et al. proposed the “fast gradient sign method” where added perturbations are determined by the gradients of the cost function with respect to inputs

Grosse et al. used the iterative algorithm add some adversarial perturbations to Android malware on about 545 thousand binary features

## 论文方案：

API one-hot vector，一个API序列是矩阵，API个数*序列长度

用substitute RNN（结构：Bi-RNN + ATT）来模拟黑盒RNN；generative model（结构：recurrent layer+decoder layer(生成小序列)）生成新序列。

训练substitute RNN时输入benign sequence ，和黑盒做交叉墒，loss Ls。

![](https://ws3.sinaimg.cn/large/006tNc79ly1fsvtfqvk0uj30fo02gmx6.jpg)

![](https://ws1.sinaimg.cn/large/006tNc79ly1fsvtbi65ayj30o80fota6.jpg)



训练generative model时，输入malware API sequence，每个API特征输出一个小API序列，接到原来API特征后面。输出的这个小API序列，用Gumbel-Softmax抽样(为了让generative到subtitue可微)，输入substitute RNN去算被预测成恶意软件的概率ps。loss就是最小化ps+一个正则项限制插入的API的个数。

![](https://ws1.sinaimg.cn/large/006tNc79ly1fsvtinttbsj30du01eq2z.jpg)

![](https://ws2.sinaimg.cn/large/006tNc79ly1fsvt6nup46j31d60wujy2.jpg)	
![](https://ws2.sinaimg.cn/large/006tNc79ly1fsvti47b93j30z60aqgon.jpg)	

## 论文实验：

victim RNN: LSTM | BiLSTM | LSTM-Average | BiLSTM-Average | LSTM- ATT |BiLSTM-ATT

## 总结			

1. 以前的解决方法，我们可以拿来比较
2. 顺序的表示方式和nlp相似，不知道可不可以word2vec结合
3. generative的结构见下一篇 Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation


