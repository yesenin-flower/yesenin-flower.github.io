[主动学习中的QBC算法](https://blog.csdn.net/qq_37735796/article/details/78391336)

通过一定的学习算法，选择出最可能含有大量信息的数据，通过主动询问用户或“专家”进行标签，将标签后的数据添加到已标签数据中，从而重新训练分类器，达到用最少的训练样本获得最大的分类精度的目的。这种类型的迭代监督学习被称为主动学习。

委员会查询（Query by Committees）： 
QBC主动学习算法利用一些由已标签的数据训练的分类器组成一个“委员会”—— C = {θ [1], … , θ[C]} ，利用委员会中各个“委员”对未标记样本的输出进行投票，然后选择投票结果“最不一致”的样本作为候选样本进行类别标记。 
QBC框架背后的基本前提是最小化版本空间(vision space)，它是由当前已标签的训练数据的假设分类的集合。如果我们把机器学习视作在版本空间中寻找最优的模型，那么主动学习的目标就是尽可能用最少的标签样本束缚这个空间的大小，而这恰恰是QBC询问输入空间中的不确定区域的目标。 

**QBC算法的基本步骤如下：** 

1. 准备数据：在实际使用中，我们有小部分已知标签的数据作为初始训练数据，还有大量的未标记的数据作为待选择标记的数据，用于由委员会评价选出最具信息量的数据来由”专家“标记后加入到训练数据集
2. 训练“主席”：准备好数据后就要开始训练分类器了，用(1)中设置的 **全部** 初始训练样本L作为训练样本来训练分类器，这个分类器是用于最终的分类；分类器的类型你可以自己尝试，好用就行；
3. 训练“委员会”：委员会是由几个不同的分类器组成，怎么让分类器不同呢?可以从训练数据入手，(1)中设置了“初始训练样本L”,只需要从初始训练样本中随机选择3/4的或者1/2的这些样本作为训练子集，用于训练委员会成员，由于每一个委员会成员的训练子集都是随机选择的，所以训练出来的分类器就有一些差别。
4. 让委员会成员分别对(1)中设置的待分类样本Q进行预测分类，再采用投票熵、相对熵（KL散度）等方法计算分歧度（分歧度最大的往往是信息量最大的，不过也有例外），选择出分歧度最大的样本，并给出相应的标签并加入到训练集中；
5. 重新训练”主席“，用测试数据测试，如果**分类准确度**达到要求就结束，否则重复步骤(3)-(5)

-----

[Combining Distant and Partial Supervision for Relation Extraction](https://nlp.stanford.edu/pubs/2014emnlp-kbpactivelearning.pdf)

QBC For MIML-RE

1. “委员会”：有放回抽样训练7次。
2. 用JS散度计算分歧度。
3. MIML-RE是用EM算法。E-step算潜在的z值，M-step更新z和y分类器的全总。


训练多个分类器，对instances 预测，用KL散度计算所有分类器最不确定的instance，然后给这个instance做标注，最后放回到训练集中			
​		
​				
​		
​	
